{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Boolean_Retrieval_System.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWypQ8w8W9yB",
        "outputId": "6a9abbc0-45b3-40c4-e5fa-2ae3f154374c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from collections import deque\n",
        "import nltk\n",
        "import re\n",
        "import os\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PRE-PROCESSING**\n",
        "\n",
        "*   Tokenizing\n",
        "*   Spl chars and digits Removal \n",
        "*   Stopword Removal \n",
        "*   Case folding \n",
        "*   Stemming\n",
        "*   Creating postings lists\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gAaSk3yYkjNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import PorterStemmer\n",
        "doc_map = dict()\n",
        "vocab = set()\n",
        "posting_list = defaultdict(list)\n",
        "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "JorjMS5qagay"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(fpath):\n",
        "  corpus = fpath\n",
        "  docId=1\n",
        "  for fname in (os.listdir(corpus)):\n",
        "    if(os.path.isdir(corpus+\"/\"+fname)):\n",
        "      continue\n",
        "    with open(corpus+\"/\"+fname,\"r\") as f:\n",
        "      txt = f.read()\n",
        "    #Removing digits and other special chars\n",
        "    txt = re.sub(re.compile(r\"[^a-zA-Z0-9\\s']\"),\"\",txt)\n",
        "    txt = re.sub(re.compile(r\"\\d\"),\"\",txt)\n",
        "    # Tokenizing text\n",
        "    wordlist = nltk.tokenize.word_tokenize(txt)\n",
        "    \n",
        "    # Case folding - converting all words to lower case and removing stopwords\n",
        "    wordlist = [w.lower() for w in wordlist if w not in stopwords]\n",
        "    \n",
        "    # Stemming using PorterStemmer algorithm\n",
        "    wordlist = [stemmer.stem(w) for w in wordlist]\n",
        "    \n",
        "    # term list\n",
        "    dict_terms = list(set(wordlist))\n",
        "    \n",
        "    # creating the inverted index data structure\n",
        "    for t in dict_terms:\n",
        "      posting_list[t].append(docId)\n",
        "\n",
        "    # mapping docId==>docPath\n",
        "    doc_map[docId] = os.path.basename(fname) \n",
        "\n",
        "    docId+=1\n",
        "\n"
      ],
      "metadata": {
        "id": "Q1nYfCHbbWMU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzipping the corpus zip file\n",
        "!unzip \"/content/shakespeares-works_TXT_FolgerShakespeare (1).zip\" -d \"/content/corpus\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MxY5iAIfDf8",
        "outputId": "0f8837c5-74c1-4bdf-8c08-f2a63eacef91"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/shakespeares-works_TXT_FolgerShakespeare (1).zip\n",
            "  inflating: /content/corpus/much-ado-about-nothing_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/richard-iii_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/the-winters-tale_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/richard-ii_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/henry-vi-part-3_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/the-two-noble-kinsmen_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/venus-and-adonis_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/timon-of-athens_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/the-merchant-of-venice_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/loves-labors-lost_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/troilus-and-cressida_TXT_FolgerShakespeare.txt  \n",
            "   creating: /content/corpus/__MACOSX/\n",
            "  inflating: /content/corpus/__MACOSX/._troilus-and-cressida_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/a-midsummer-nights-dream_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/lucrece_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/henry-iv-part-1_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/henry-vi-part-1_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/henry-v_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/__MACOSX/._henry-v_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/pericles_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/the-merry-wives-of-windsor_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/as-you-like-it_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/king-john_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/cymbeline_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/alls-well-that-ends-well_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/henry-viii_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/julius-caesar_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/the-tempest_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/macbeth_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/hamlet_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/__MACOSX/._hamlet_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/the-taming-of-the-shrew_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/coriolanus_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/othello_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/shakespeares-sonnets_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/romeo-and-juliet_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/measure-for-measure_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/antony-and-cleopatra_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/__MACOSX/._antony-and-cleopatra_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/henry-vi-part-2_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/titus-andronicus_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/twelfth-night_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/henry-iv-part-2_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/king-lear_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/the-comedy-of-errors_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/the-phoenix-and-turtle_TXT_FolgerShakespeare.txt  \n",
            "  inflating: /content/corpus/the-two-gentlemen-of-verona_TXT_FolgerShakespeare.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=\"/content/corpus\" # give path of location of folder containing the docs\n",
        "preprocess(corpus)\n",
        "# Creating vocabulary set from posting_list keys\n",
        "vocab = set(posting_list.keys())"
      ],
      "metadata": {
        "id": "n4oyESoMzybx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Spell check**"
      ],
      "metadata": {
        "id": "v-E_sjw1myXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculates edit distance between 2 words w1 and w2\n",
        "def edit_dist(w1,w2):\n",
        "  len1 = len(w1)\n",
        "  len2 = len(w2)\n",
        "  if(len1==0):\n",
        "    return len2\n",
        "  if(len2==0):\n",
        "    return len1\n",
        "  \n",
        "  dp = [[0 for x in range(len2+1)] for x in range(len1+1)]\n",
        "\n",
        "  for i in range(len1+1):\n",
        "    for j in range(len2+1):\n",
        "      if(i ==0):\n",
        "        dp[i][j]=j\n",
        "      elif(j==0):\n",
        "        dp[i][j]=i\n",
        "      else:\n",
        "        if(w1[i-1]!=w2[j-1]):\n",
        "          dp[i][j] = min({dp[i-1][j]+1,dp[i][j-1]+1,dp[i-1][j-1]+1})\n",
        "        else:\n",
        "          dp[i][j] = dp[i-1][j-1]\n",
        "\n",
        "\n",
        "  return dp[len1][len2]"
      ],
      "metadata": {
        "id": "4kRzdHnEnmhr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scans the vocab and returns the word having minimum edit distance from w (used to correct the spelling)\n",
        "def correct_spelling(w):\n",
        "  if(isOperator(w)):\n",
        "    return w\n",
        "  mn_dist = np.inf\n",
        "  correct_word=\"\"\n",
        "  for word in vocab:\n",
        "    dist = edit_dist(w,word)\n",
        "    if(dist<mn_dist):\n",
        "      mn_dist=dist\n",
        "      correct_word=word\n",
        "  \n",
        "  return correct_word\n"
      ],
      "metadata": {
        "id": "HvgpIfRpruRi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Wild card Searching**\n"
      ],
      "metadata": {
        "id": "24qmyZik6D23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating permuterm index : maps rotated terms ==> original terms\n",
        "permuterm = dict()\n",
        "keys = posting_list.keys()\n",
        "for k in sorted(keys):\n",
        "  pkey = k + \"$\"\n",
        "  for pos in range(len(pkey),0,-1):\n",
        "    rot_key = pkey[pos:]+pkey[:pos]\n",
        "    permuterm[rot_key] = k"
      ],
      "metadata": {
        "id": "_Iop0MbU6L22"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pref_match(w):\n",
        "  matched_terms=[]\n",
        "  for keys in permuterm.keys():\n",
        "    if keys.startswith(w):\n",
        "      matched_terms.append(permuterm[keys])\n",
        "  return matched_terms"
      ],
      "metadata": {
        "id": "9V2YTx5PEGdm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def match_terms(q):\n",
        "  word=\"\"\n",
        "  wa=\"\"\n",
        "  wb=\"\"\n",
        "  # splitting a given word with * as delimiter\n",
        "  parts = q.split('*')\n",
        "  if len(parts) == 3:\n",
        "    case=4\n",
        "  elif parts[1]=='':\n",
        "    case=1\n",
        "  elif parts[0]=='':\n",
        "    case=2\n",
        "  elif parts[0]!='' and parts[1]!='':\n",
        "    case=3\n",
        "  \n",
        "  if case==4:\n",
        "    if parts[0] == '':\n",
        "      case=1\n",
        "  \n",
        "  if case==1:\n",
        "    # terms of form A*\n",
        "    word = \"$\"+parts[0]\n",
        "  elif case==2:\n",
        "    # terms of form *A\n",
        "    word = parts[1] + \"$\"\n",
        "  elif case==3:\n",
        "    # terms of form A*B\n",
        "    word = parts[1] + \"$\" + parts[0]\n",
        "  elif case==4:\n",
        "    # terms of form A*B*C\n",
        "    wa = parts[2] + \"$\" + parts[0]\n",
        "    wb = parts[1]\n",
        "\n",
        "  possible_terms = \"\"\n",
        "  if case!=4:\n",
        "    lst = pref_match(word)\n",
        "  else:\n",
        "    lst = list(set(pref_match(wa)).intersection(set(pref_match(wb)))) \n",
        "  \n",
        "  if(len(lst)==1):\n",
        "      possible_terms=lst[0]\n",
        "  elif(len(lst)>0):\n",
        "    possible_terms += \"(\"\n",
        "    for i in lst: \n",
        "      possible_terms += (\" \" + i +\" | \")\n",
        "    possible_terms=possible_terms.rstrip(possible_terms[-1])\n",
        "    possible_terms=possible_terms.rstrip(possible_terms[-1])\n",
        "    possible_terms += \")\"\n",
        "\n",
        "  #print(word)\n",
        "  #print(wa)\n",
        "  #print(wb)\n",
        "  #print(possible_terms)\n",
        "  return possible_terms"
      ],
      "metadata": {
        "id": "4nu5Z2n18T5z"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "match_terms(\"ca*l*ia\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Zah1jpDAFu0X",
        "outputId": "0317431e-2391-452f-e5e2-3326d5f7a388"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'( cavalleria |  calphurnia |  castalia )'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Document matching**"
      ],
      "metadata": {
        "id": "gGi30gy1E2Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list of doc ids\n",
        "docIdlist=doc_map.keys()"
      ],
      "metadata": {
        "id": "Tx0c9y_3adKu"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining precedence of boolean operators\n",
        "precedence = dict()\n",
        "precedence['~']=3\n",
        "precedence['&']=2\n",
        "precedence['|']=1"
      ],
      "metadata": {
        "id": "SDw4EtRfRpON"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocesses the query before finding the matching documents\n",
        "def query_preprocess(text):\n",
        "  #text = re.sub(re.compile(r\"[^a-zA-Z0-9\\s]\"),\"\",text)\n",
        "  text = re.sub(re.compile(r\"\\d\"),\"\",text)\n",
        "  text = text.lower()\n",
        "  # Tokenizing text\n",
        "  query_tokens = nltk.tokenize.word_tokenize(text)\n",
        "  query_tokens = [correct_spelling(w) for w in query_tokens]\n",
        "  return query_tokens\n"
      ],
      "metadata": {
        "id": "B7JdQEvjFjre"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def isOperator(ch):\n",
        "  if(ch=='&' or ch=='|' or ch=='~' or ch=='(' or ch==')'):\n",
        "    return True\n",
        "  return False"
      ],
      "metadata": {
        "id": "r9ghGMPPObad"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_op(left,right,op):\n",
        "  result=[]\n",
        "  if(op=='~'):\n",
        "    for i in docIdlist:\n",
        "      if(i not in left):\n",
        "        result.append(i)\n",
        "  elif(op=='&'):\n",
        "    temp = set(right)\n",
        "    result = [val for val in left if val in temp]\n",
        "  elif(op=='|'):\n",
        "    result = list(set(left)|set(right))\n",
        "  \n",
        "  return result\n"
      ],
      "metadata": {
        "id": "UcH2jqmnYgEB"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluates the query and returns ids of matched docs\n",
        "def evaluate_and_match(query_tokens):\n",
        "  operand_stack = deque()\n",
        "  operator_stack = deque()\n",
        "  result=[]\n",
        "  for t in query_tokens:\n",
        "    if(isOperator(t)):\n",
        "      if(t=='('):\n",
        "        operator_stack.append(t)\n",
        "      elif(t==')'):\n",
        "        while((operator_stack) and (operator_stack[-1]!='(')):\n",
        "          op = operator_stack[-1]\n",
        "          operator_stack.pop()\n",
        "          if(op=='~'):\n",
        "            right = operand_stack[-1]\n",
        "            operand_stack.pop()\n",
        "            operand_stack.append(eval_op(right,right,op))\n",
        "          else:\n",
        "            right = operand_stack[-1]\n",
        "            operand_stack.pop()\n",
        "            left = operand_stack[-1]\n",
        "            operand_stack.pop()\n",
        "            result = eval_op(left,right,op)\n",
        "            operand_stack.append(result)\n",
        "        operator_stack.pop()\n",
        "      else:\n",
        "        while((operator_stack) and (operator_stack[-1]=='&' or operator_stack[-1]=='|' or operator_stack[-1]=='~') and (precedence[operator_stack[-1]]>precedence[t])):\n",
        "          op = operator_stack[-1]\n",
        "          operator_stack.pop()\n",
        "          if(op=='~'):\n",
        "            right = operand_stack[-1]\n",
        "            operand_stack.pop()\n",
        "            operand_stack.append(eval_op(right,right,op))\n",
        "          else:\n",
        "            right = operand_stack[-1]\n",
        "            operand_stack.pop()\n",
        "            left = operand_stack[-1]\n",
        "            operand_stack.pop()\n",
        "            result = eval_op(left,right,op)\n",
        "            operand_stack.append(result)\n",
        "        operator_stack.append(t)\n",
        "    else:\n",
        "      token = stemmer.stem(t)\n",
        "      operand_stack.append(posting_list[token])\n",
        "  while(operator_stack):\n",
        "    op = operator_stack[-1]\n",
        "    operator_stack.pop()\n",
        "    if(op=='~'):\n",
        "      right = operand_stack[-1]\n",
        "      operand_stack.pop()\n",
        "      operand_stack.append(eval_op(right,right,op))\n",
        "    else:\n",
        "      right = operand_stack[-1]\n",
        "      operand_stack.pop()\n",
        "      left = operand_stack[-1]\n",
        "      operand_stack.pop()\n",
        "      result = eval_op(left,right,op)\n",
        "      operand_stack.append(result)\n",
        "  return operand_stack[0]\n",
        "    "
      ],
      "metadata": {
        "id": "_UReBS8wOCZe"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Given the matched docId list, returns document names \n",
        "def get_docs(lst):\n",
        "  docs= []\n",
        "  for i in lst:\n",
        "    docs.append(doc_map[i])\n",
        "  \n",
        "  return docs"
      ],
      "metadata": {
        "id": "HU6iw4KytC34"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Querying**\n",
        "Expects a *well formed query* to be given as input\n",
        "\n",
        "**Well-formed query:** \n",
        "\n",
        "*   Every word/symbol must be space seperated\n",
        "*   The following symbols represent boolean operators : **AND = & , OR = | , NOT = ~**\n",
        "*   If parenthesis are used, it must be ensured that they are properly balanced\n",
        "*   In absence of parenthesis, the precedence order followed by operators is ~ > & > |\n",
        "* Supports wildcard entries/terms of following formats: A\\* , \\*A , A\\*B , A\\*B\\*C \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fD1GYinMtZy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = input(\"Enter query: \")\n",
        "for term in query.split(\" \"):\n",
        "  if('*' in term):\n",
        "    query=query.replace(term,match_terms(term))\n",
        "q_tokens = query_preprocess(query)"
      ],
      "metadata": {
        "id": "Oo8Wt1QNjt93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0faa8618-c965-433f-d4c5-672bb39d0d41"
      },
      "execution_count": 119,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter query: brut* & br*oth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ML4X0QvxMq-d",
        "outputId": "24336a2b-76db-4adc-a7d4-b7356570da1e"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['(',\n",
              " 'brute',\n",
              " '|',\n",
              " 'brutethen',\n",
              " '|',\n",
              " 'brutish',\n",
              " '|',\n",
              " 'brutishgo',\n",
              " '|',\n",
              " 'brutu',\n",
              " '|',\n",
              " 'brutusnoth',\n",
              " ')',\n",
              " '&',\n",
              " '(',\n",
              " 'broth',\n",
              " '|',\n",
              " 'brutusnoth',\n",
              " ')']"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids=evaluate_and_match(q_tokens)"
      ],
      "metadata": {
        "id": "niD3O327gH3j"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns list of names of documents satisfying the query\n",
        "get_docs(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uJx1JDdxLMG",
        "outputId": "3cfbe4da-8528-433a-bcad-437fea015c4d"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['julius-caesar_TXT_FolgerShakespeare.txt',\n",
              " 'the-merchant-of-venice_TXT_FolgerShakespeare.txt',\n",
              " 'henry-v_TXT_FolgerShakespeare.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    }
  ]
}